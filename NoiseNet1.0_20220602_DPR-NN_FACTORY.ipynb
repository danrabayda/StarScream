{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a780e5-0cbf-4311-ad0d-73cb7ffcf890",
   "metadata": {},
   "source": [
    "# The Factory\n",
    "A Model Optimization Method (or MOM)\n",
    "\n",
    "This is a \"factory\" designed to utilize optuna and labeled data to find the most accurate model yielding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007f52df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8000,
     "status": "ok",
     "timestamp": 1649087474234,
     "user": {
      "displayName": "Daniel Rabayda",
      "userId": "03984531504935332105"
     },
     "user_tz": 240
    },
    "id": "007f52df",
    "outputId": "997385d6-e782-4dc9-a400-73ade9775b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 2.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Basic Python Functionality\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "import copy\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "from os import walk\n",
    "import scipy.io.wavfile\n",
    "from scipy import signal\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Import Python Libraries for Machine Learning\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, calinski_harabasz_score, v_measure_score\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize, LabelBinarizer, minmax_scale\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import pairwise_distances, pairwise_distances_argmin_min\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Import Keras/Tensorflow Libraries for Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, Callback\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D, Flatten, Activation, Lambda, Convolution1D, Dense, add, Lambda\n",
    "from tensorflow.keras.applications import xception\n",
    "import tensorflow_addons as tfa  \n",
    "\n",
    "\n",
    "# GUI Components\n",
    "from pydub.playback import play\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from tkinter import * \n",
    "from tkinter import ttk\n",
    "from tkinter.ttk import *\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "\n",
    "# Factory Components\n",
    "import optuna\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "def vdir(directory): #verify a directory exists, if not make it\n",
    "    if not os.path.exists(directory): os.mkdir(directory)\n",
    "    return directory\n",
    "\n",
    "data_folder=vdir('data')\n",
    "more_folder=vdir('more')\n",
    "downloads_folder=vdir(data_folder+'/downloads')\n",
    "npys_folder=vdir(data_folder+'/npys')\n",
    "time_series_npys_folder=vdir(npys_folder+'/time_series_audio_npys')    \n",
    "audio_set_npys_folder=vdir(npys_folder+'/audio_set_npys')\n",
    "\n",
    "time_series_downloads_folder=vdir(downloads_folder+'/time_series_audio_mp4s') #long time-series mp4 files go in here\n",
    "audio_set_downloads_folder=vdir(downloads_folder+'/audio_set_mp4s') #short mp4 files from google's AudioSet data go here\n",
    "\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "Train_from_scratch = True\n",
    "\n",
    "print(\"tf version:\",tf.__version__)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d28724",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#   DATA SETUP\n",
    "###\n",
    "\n",
    "#Instantiation\n",
    "classes = [\"Motorboat_speedboat\",\n",
    "\"Racecar_autoracing\",\n",
    "\"Carpassingby\",\n",
    "\"Tiresqueal\",\n",
    "\"Caralarm\",\n",
    "\"Vehiclehorn_carhorn_honking\",\n",
    "\"Airbrake\",\n",
    "\"Airhorn_truckhorn\",\n",
    "\"Bus\",\n",
    "\"Motorcycle\",\n",
    "\"Trafficnoise_roadwaynoise\",\n",
    "\"Railroadcar_trainwagon\",\n",
    "\"Trainwheelssquealing\",\n",
    "\"Helicopter\",\n",
    "\"Fixed-wingaircraft_airplane\",\n",
    "\"Lightengine(highfrequency)\",\n",
    "\"Mediumengine(midfrequency)\",\n",
    "\"Heavyengine(lowfrequency)\"]\n",
    "pdict = {}\n",
    "pdict['r_smp'] = 44100\n",
    "\n",
    "if not os.path.exists(more_folder+'/fltc.npy'):\n",
    "\n",
    "    #Load the raw data files and convert them if needed\n",
    "    video_files=np.sort(os.listdir(audio_set_downloads_folder))\n",
    "    for i,video_file in enumerate(video_files):\n",
    "        print(int(100*i/len(video_files)),\"% converting      \",end='\\r')\n",
    "        video_name=''.join(video_file.split('.')[:-1])\n",
    "        audio_file = os.path.join(audio_set_npys_folder, video_name+f'_{pdict[\"r_smp\"]}.npy')\n",
    "        if not os.path.exists(audio_file):\n",
    "            try:\n",
    "                seg = AudioSegment.from_file(audio_set_downloads_folder+'/'+video_file, format='mp4')\n",
    "                samples = pydub_to_np(seg, pdict['r_smp'])\n",
    "                np.save(audio_file, samples)\n",
    "            except:\n",
    "                print(\"\\nError with \"+video_name+\"\\n\")  # to handle exception   \n",
    "    print(\"100 % converted      \")\n",
    "    test_numpy_files = [os.path.join(audio_set_npys_folder, f) for f in os.listdir(audio_set_npys_folder) if Path(f).suffix == '.npy']\n",
    "\n",
    "    #Import and format labels and times:\n",
    "    files_labels_times_raw = pd.read_csv('clean_download.csv')[['segment_id','name','time_interval']].to_numpy()\n",
    "    files_raw=[audio_set_npys_folder+'/'+files_labels_times_raw[:,0][i]+'_'+str(pdict['r_smp'])+'.npy' for i in range(len(files_labels_times_raw))]\n",
    "    labels_raw=[files_labels_times_raw[:,1][i][1:-1].replace(' ','').replace(\"'\",\"\").split(',') for i in range(len(files_labels_times_raw))]\n",
    "    initial_length=len(files_raw) #8690\n",
    "    times_raw_str=[files_labels_times_raw[:,2][i].replace(' ','').replace('(','').replace(')','').strip('][').split(',') for i in range(len(files_labels_times_raw))]\n",
    "    times_raw=[[[int(float(tlist[i])*pdict['r_smp']),int(float(tlist[i+1])*pdict['r_smp'])] for i in range(len(tlist)//2)] for tlist in times_raw_str]\n",
    "    #times_raw is now preformatted into time in samples\n",
    "\n",
    "    #Pop nonexistent files and their corresponding labels:\n",
    "    if os.path.exists(more_folder+'/pop_arr.npy'):\n",
    "        pop_arr=np.load(more_folder+'/pop_arr.npy')\n",
    "        lengths_tmp=np.load(more_folder+'/file_lengths.npy')\n",
    "        [[files_raw.pop(i), labels_raw.pop(i), times_raw.pop(i)] for i in pop_arr]\n",
    "    else:\n",
    "        pop_arr=[]\n",
    "        i,j,count=0,0,0\n",
    "        while j<len(files_raw):\n",
    "            if np.sort(test_numpy_files)[i]==files_raw[j]:\n",
    "                i+=1\n",
    "                j+=1\n",
    "            else:\n",
    "                count+=1\n",
    "                pop_arr.append(j)\n",
    "                j+=1\n",
    "        pop_arr.reverse()\n",
    "        for popi in pop_arr:\n",
    "            files_raw.pop(popi)\n",
    "            labels_raw.pop(popi)\n",
    "            times_raw.pop(popi)\n",
    "        assert len(files_raw)==initial_length-count #if this raises an error then something is not in the correct order (check the sorting of files_raw and test_numpy_files)\n",
    "\n",
    "        #also pop files under specified video length (in s)\n",
    "        tmp_data=[np.load(f) for f in files_raw]\n",
    "        lengths_tmp=[len(tmp_data[i])/pdict['r_smp'] for i in range(len(tmp_data))]\n",
    "        lengths_under_9s=[i for i,length in enumerate(lengths_tmp) if length<9]; lengths_under_9s.reverse()\n",
    "        [[files_raw.pop(i), labels_raw.pop(i), times_raw.pop(i),lengths_tmp.pop(i)] for i in lengths_under_9s]\n",
    "        pop_arr.extend(lengths_under_9s) #the way the indices were popped originally must remain the same order so long story short don't sort this list\n",
    "        lengths_tmp=[len(np.load(f)) for f in files_raw]\n",
    "        np.save(more_folder+'/pop_arr.npy',pop_arr)\n",
    "        np.save(more_folder+'/file_lengths',lengths_tmp)\n",
    "        del tmp_data\n",
    "\n",
    "    #Reformat the times\n",
    "    for i in range(len(times_raw)):\n",
    "        for j in range(len(times_raw[i])):\n",
    "            [tmp_ll,tmp_ul]=times_raw[i][j] #upper and lower limits are the event time tags (in samples)\n",
    "            if tmp_ll>tmp_ul: #if our lower limit is greater than the upper limit\n",
    "                tmp_ll,tmp_ul=tmp_ul,tmp_ll #flip them\n",
    "            if tmp_ul>lengths_tmp[i]: #if the true upper limit goes beyond the length of our file\n",
    "                tmp_ul=lengths_tmp[i] #set it equal to the length of the file\n",
    "            times_raw[i][j]=[tmp_ll,tmp_ul]\n",
    "\n",
    "    #Reformat the labels\n",
    "    labels_c=[[classes.index(labels_raw[j][i]) for i in range(len(labels_raw[j]))] for j in range(len(labels_raw))] #write them as class indices instead of strings every time. This is important for our generator later\n",
    "\n",
    "    labels=np.zeros((len(labels_raw),len(classes))) #I did this with labels_raw so it would be easier to show the below demonstration but you could do the same exact thing with labels_c instead\n",
    "    for i,label_list in enumerate(labels_raw):\n",
    "        for j in range(len(label_list)):\n",
    "            labels[i,classes.index(label_list[j])]=1\n",
    "\n",
    "    #Create class_groups\n",
    "    #This is grouping each \"file\" into 18 different lists (classes). In this method one \"file\" may also be in multiple groups (I say \"file\" because these are really indices of the files_raw array)\n",
    "    class_labels=np.identity(len(classes)) #an array of all the classes in our same format\n",
    "    class_groups=[] #class_groups is a list of lists of each classes common file indices for data_array[index]. for example index 2 and 4 both contain 'Vehiclehorn_carhorn_honking' so we group them together in the same class_group\n",
    "    for i in range(len(class_labels)):\n",
    "        tmp_group=[]\n",
    "        for j in range(len(labels)):\n",
    "            if np.isin(2,class_labels[i]+labels[j]):\n",
    "                tmp_group.append(j)\n",
    "        class_groups.append(tmp_group)\n",
    "\n",
    "    length_class_groups=[len(class_groups[i]) for i in range(len(class_groups))]\n",
    "\n",
    "    np.save(more_folder+'/fltc.npy',np.array([files_raw,labels_c,times_raw,class_groups],dtype='object'))\n",
    "else:\n",
    "    [files_raw,labels_c,times_raw,class_groups]=np.load(more_folder+'/fltc.npy',allow_pickle=True)\n",
    "\n",
    "def fltdc(lb,ub):\n",
    "    f_=files_raw[lb:ub]\n",
    "    l_=labels_c[lb:ub]\n",
    "    t_=times_raw[lb:ub]\n",
    "    d_=[np.load(f) for f in f_]\n",
    "    c_=[[i-lb for i in class_groups[j] if i<ub and i>=lb] for j in range(len(class_groups))] #if lb was 0 always then the inner part would just be [i for i in class_groups[j] if i<ub]\n",
    "    return f_,l_,t_,d_,c_\n",
    "\n",
    "#Training data set\n",
    "lb1,ub1=0,2400 #highest ub is len(files_raw)\n",
    "\n",
    "smallFilenames,smallLabels,smallTimes,smallData,smallClassgroups=fltdc(lb1,ub1)\n",
    "lengths_tmp=[len(smallData[i])/pdict['r_smp'] for i in range(len(smallData))]\n",
    "assert np.min(lengths_tmp)>9.0 #if you changed the threshold earlier ignore this\n",
    "\n",
    "#Test and Validation data sets\n",
    "l=400 #size of each set\n",
    "\n",
    "if ub1+l+l<=len(files_raw):\n",
    "    lb2,ub2=ub1,ub1+l\n",
    "    lb3,ub3=ub2,ub2+l\n",
    "else:\n",
    "    rng=np.random.randint(len(files_raw)-l,size=2)\n",
    "    lb2,ub2=rng[0],rng[0]+l\n",
    "    lb3,ub3=rng[1],rng[1]+l\n",
    "smallFilenames2,smallLabels2,smallTimes2,smallData2,smallClassgroups2=fltdc(lb2,ub2)\n",
    "smallFilenames3,smallLabels3,smallTimes3,smallData3,smallClassgroups3=fltdc(lb3,ub3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8955d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generator C\n",
    "\n",
    "Uses labelled data and grabs nK random windows from each label or random labels (depending on how big nP is and how small the data set is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4cc965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatorC(data_array,pdict,sm_labels,sm_times,sm_class_groups,return_startpoints=False): \n",
    "    \n",
    "    nP=pdict['numP']\n",
    "    nK=pdict['numK']\n",
    "    window_size_secs=pdict['w_sec']\n",
    "    samps_per_second=pdict['r_smp']\n",
    "    v_res=pdict['vres']\n",
    "    lrb=pdict['LRB']\n",
    "    noverlap=v_res/8\n",
    "    lr=L_R(lrb)\n",
    "    \n",
    "    window_samps=int(window_size_secs*samps_per_second)\n",
    "    \n",
    "    t_len=int((window_samps-noverlap)/(v_res-noverlap))   #These are the time and frequency dimensions of my plot. They come from the scipy spectrogram generation formulas.\n",
    "    f_len=int(.75 * v_res/2)+1\n",
    "    \n",
    "    bs=nP*nK #batch size\n",
    "    \n",
    "    skip_n=nP-(len(sm_class_groups)-sm_class_groups.count([]))<=0 #if true we skip the independent picks process and just let repeated labels occur because otherwise we'd have a hard limit of nP=18\n",
    "\n",
    "    while 1:\n",
    "        \n",
    "        startpoints=[]\n",
    "        labels_gen=[]\n",
    "        spectrograms=np.zeros((bs,f_len,t_len))\n",
    "        \n",
    "        for p in range(nP):\n",
    "            sm_i,tmp_cgroup=0,[] \n",
    "            while tmp_cgroup==[] or (sm_i in labels_gen if skip_n else None): #skip_n is in there incase nP is greater than the \n",
    "                sm_i=np.random.randint(len(sm_class_groups))\n",
    "                tmp_cgroup=sm_class_groups[sm_i] #choose a random non-empty non-repeated segment (the larger rng is the less time this takes)\n",
    "            #i will be our label and tmp_cgroup will be our data_array indices to randomly pick windows from \n",
    "            np.random.shuffle(tmp_cgroup)\n",
    "            \n",
    "            #now sm_i is our current class and tmp_cgroup is our current classgroup \n",
    "            for k in range(nK):\n",
    "                batch_idx=p*nK+k\n",
    "                dind=tmp_cgroup[k] if k<len(tmp_cgroup) else np.random.choice(tmp_cgroup) #we want a random choice of the current classgroup (a random file from that class) but we don't want repeated files if we can avoid it so this is an ordered selection of the random shuffled tmp_cgroup until it runs out of options, then its just a random choice\n",
    "                carr=data_array[dind]\n",
    "                \n",
    "                #Here we choose a window based around our time segment data\n",
    "                #The way I approached this was to imagine two cases, one where the window is larger than the event time frame and one where the window is shorter than that event time frame\n",
    "                t_ind=np.random.choice([i for i,smsmlbs in enumerate(sm_labels[dind]) if smsmlbs==sm_i])#randomly chooses a valid labels index where the current class exists in that file\n",
    "                [ll,ul]=sm_times[dind][t_ind] #lower and upper limits of the event time frame\n",
    "                \n",
    "                if ll+window_samps>len(carr): #if the event time frame is smaller than the window and near the end of the file we need to make sure our window doesn't just go right off the end of the file\n",
    "                    [tl,tu]=np.sort([len(carr)-window_samps,ul-window_samps])\n",
    "                elif ul-window_samps<0: #if the event time frame is smaller than the window and closeto the beginning of the file we need to make sure our window doesn't precede 0 \n",
    "                    [tl,tu]=np.sort([ll,0])\n",
    "                else:\n",
    "                    [tl,tu]=np.sort([ll,ul-window_samps])#let's take care of 2 possibilities at once, one the window is smaller than the event time frame (if window_samps<ul-ll then we want samp=np.random.randint(ll,ul-window_samps)), and two the window is larger than the event time frame(if window_samps>ul-ll then we want samp=np.random.randint(ul-window_samps,ll))\n",
    "                samp=np.random.randint(tl,tu) if tl!=tu else tl #if tl==tu then np.random.randint will give an error so we just give it the exception\n",
    "                window=carr[samp:samp+window_samps,xl(lr)]\n",
    "                \n",
    "                n_window=window/(np.max(np.abs(window))+1e-8)\n",
    "\n",
    "                labels_gen.append(sm_i)\n",
    "                startpoints.extend([dind,samp])\n",
    "                \n",
    "                frequencyx, timesx, spectrogramx = signal.spectrogram(n_window, samps_per_second, nperseg=v_res)\n",
    "                log_spectrogram=np.log10(spectrogramx[:f_len,:t_len],out=spectrogramx[:f_len,:t_len],where=spectrogramx[:f_len,:t_len] > 0)\n",
    "                diff=np.max(log_spectrogram)-np.min(log_spectrogram)\n",
    "                spectrograms[batch_idx]=(log_spectrogram-np.min(log_spectrogram))/diff if diff!=0 else log_spectrogram-np.min(log_spectrogram)\n",
    "\n",
    "        spectrograms_out=cm.viridis(spectrograms)[:,:,:,0:3]\n",
    "\n",
    "        \n",
    "        if return_startpoints:\n",
    "            yield spectrograms_out, np.array(labels_gen).astype(int), startpoints\n",
    "        else:\n",
    "            yield spectrograms_out, np.array(labels_gen).astype(int)\n",
    "\n",
    "#Additional Functions:\n",
    "def xl(lr):\n",
    "    return np.random.randint(2) if lr==2 else lr #these are split so we don't have to do string comprehension in our deepest loop\n",
    "def L_R(lrb):\n",
    "    return 0 if lrb=='L' else 1 if lrb=='R' else 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced96e2",
   "metadata": {
    "id": "8ced96e2"
   },
   "source": [
    "# NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53c677c",
   "metadata": {
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1649087546511,
     "user": {
      "displayName": "Daniel Rabayda",
      "userId": "03984531504935332105"
     },
     "user_tz": 240
    },
    "id": "b53c677c"
   },
   "outputs": [],
   "source": [
    "def encoder_network(pdict, padding='same', drop=0.05):\n",
    "    \n",
    "    nb_channels = pdict['ch']\n",
    "    dense_layers  = pdict['d']\n",
    "    out_dim = pdict['od']\n",
    "    norm = pdict['n']\n",
    "    img_height = int((int(pdict['w_sec']*pdict['r_smp'])-(pdict['vres']/8))/(pdict['vres']*7/8))\n",
    "    img_width = int(.75 * pdict['vres']/2)+1\n",
    "\n",
    "    input_layer = Input(shape=(img_width, img_height, nb_channels))\n",
    "    base_model = xception.Xception(weights='imagenet',\n",
    "                              include_top=False,\n",
    "                              input_shape=(img_width, img_height, nb_channels),\n",
    "                              pooling='avg')\n",
    "\n",
    "    x = base_model(input_layer)\n",
    "    \n",
    "    for d in dense_layers:\n",
    "        x = Dense(d, activation='relu')(x)\n",
    "        \n",
    "    x = Dense(out_dim, activation='linear', name='output_dense')(x)\n",
    "\n",
    "    # normalize output for cosine similarity\n",
    "    if norm:\n",
    "        x = Lambda(lambda xx: K.l2_normalize(xx, axis=1))(x)\n",
    "\n",
    "    output_layer = x\n",
    "    print(f'model.x = {input_layer.shape}')\n",
    "    print(f'model.y = {output_layer.shape}')\n",
    "    model = Model(input_layer, output_layer, name='encoder_model')\n",
    "\n",
    "    return model\n",
    "\n",
    "def vggish_network(pdict, padding='same'):\n",
    "    nb_channels = pdict['ch']\n",
    "    dense_layers  = pdict['d']\n",
    "    out_dim = pdict['od']\n",
    "    norm = pdict['n']\n",
    "    img_height = int((int(pdict['w_sec']*pdict['r_smp'])-(pdict['vres']/8))/(pdict['vres']*7/8))\n",
    "    img_width = int(.75 * pdict['vres']/2)+1\n",
    "    \n",
    "    input_layer = tf.keras.Input(shape=(img_width, img_height, nb_channels), name=\"img\")\n",
    "    x = tf.keras.layers.Conv2D(64, 3, data_format='channels_last', padding=padding, activation=\"relu\")(input_layer)\n",
    "    x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "    x = tf.keras.layers.Conv2D(128, 3, data_format='channels_last', padding=padding, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "    x = tf.keras.layers.Conv2D(256, 3, data_format='channels_last', padding=padding, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Conv2D(256, 3, data_format='channels_last', padding=padding, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "    x = tf.keras.layers.Conv2D(512, 3, data_format='channels_last', padding=padding, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Conv2D(512, 3, data_format='channels_last', padding=padding, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    for d in dense_layers:\n",
    "        x = Dense(d)(x) #, activation='relu'\n",
    "        \n",
    "    # normalize output for cosine similarity\n",
    "    if norm:\n",
    "        x = Lambda(lambda xx: K.l2_normalize(xx, axis=1))(x)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(out_dim, activation='linear', name='output_dense')(x)\n",
    "    model = Model(input_layer, output_layer, name=\"vggish_model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_custom_model(pdict, model_folder='models/'):\n",
    "\n",
    "    if 'iniW' in pdict.keys():\n",
    "        print(f'|time:{pdict[\"iniW\"]}')\n",
    "        model_name = [n for n in os.listdir(model_folder) if f'|time:{pdict[\"iniW\"]}' in n][0][:-3]\n",
    "        model_file = os.path.join(model_folder, model_name + '.h5')\n",
    "\n",
    "        print('loading previous model:\\n', model_file)\n",
    "        pdict = name2param(os.path.basename(model_file))\n",
    "        my_acc, my_k = get_acc(pdict)\n",
    "        my_loss = tfa.losses.TripletSemiHardLoss()\n",
    "        model = load_model(model_file, custom_objects={'triplet_loss': my_loss, 'tr_acc': my_acc, 's_knn':my_k})\n",
    "    else:\n",
    "        model_name = param2name(pdict)\n",
    "        model_file = os.path.join(model_folder, model_name)\n",
    "\n",
    "        print('building new model:\\n', model_file)\n",
    "        my_acc, my_k = get_acc(pdict)\n",
    "        my_loss = tfa.losses.TripletSemiHardLoss()\n",
    "        model = vggish_network(pdict) if pdict['vggish'] else encoder_network(pdict)\n",
    "\n",
    "    o = Adam(learning_rate=pdict['lr'], clipnorm=1.)\n",
    "                      \n",
    "    model.compile(loss=my_loss, optimizer=o, metrics=[my_acc,my_k])\n",
    "\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def get_callbacks(model_name, model_folder, log_folder):\n",
    "    pdict = name2param(os.path.basename(model_name))\n",
    "    tensor_foldername = os.path.join(log_folder, model_name)\n",
    "    model_filename = os.path.join(model_folder, model_name + '.h5')\n",
    "\n",
    "    sv = ModelCheckpoint(filepath=model_filename, monitor='val_loss', save_best_only=True,\n",
    "                         save_weights_only=False, mode='min')\n",
    "    #print(os.listdir(model_folder)) ####\n",
    "    stp = EarlyStopping(monitor='val_loss', min_delta=0, patience=pdict['pat'],\n",
    "                        verbose=0, mode='min', baseline=None)\n",
    "    tbd = TensorBoard(log_dir=tensor_foldername)\n",
    "    file_writer = tf.summary.create_file_writer(tensor_foldername + \"/metrics\")\n",
    "    file_writer.set_as_default()\n",
    "\n",
    "    return [sv, stp, tbd]\n",
    "\n",
    "\n",
    "def param2name(pdict):\n",
    "    name = []\n",
    "    for key in pdict.keys():\n",
    "        if type(pdict[key]) is list:\n",
    "            name.append(f'{key}:{\"x\".join(map(str, pdict[key]))}')\n",
    "        else:\n",
    "            name.append(f'{key}:{pdict[key]}')\n",
    "    return '|'.join(name)\n",
    "\n",
    "\n",
    "def name2param(name):\n",
    "    regnumber = re.compile(r'^\\d+(\\.\\d+)?$')\n",
    "    pdict = dict([p.split(':') for p in name.split('|')])\n",
    "    for key in pdict.keys():\n",
    "        if regnumber.match(pdict[key]):\n",
    "            try:\n",
    "                pdict[key] = int(pdict[key])\n",
    "            except:\n",
    "                pdict[key] = float(pdict[key])\n",
    "        else:\n",
    "            if 'x' in pdict[key][:-1]:\n",
    "                pdict[key] = list(map(int, pdict[key].split('x')))\n",
    "            try:\n",
    "                pdict[key] = float(pdict[key])\n",
    "            except:\n",
    "                pass\n",
    "    return pdict\n",
    "\n",
    "# Acc Function\n",
    "def get_pairwise_dists(x, num_p, num_k):\n",
    "    # pairwise distances for whole batch\n",
    "    # (redundant computation but probably still faster than alternative)\n",
    "    norms = tf.reduce_sum(x * x, 1)\n",
    "    norms = tf.reshape(norms, [-1, 1])\n",
    "    dists = norms - 2 * tf.matmul(x, x, transpose_b=True) + tf.transpose(norms)\n",
    "    dists = K.sqrt(K.relu(dists))\n",
    "\n",
    "    # get the max intra-class distance for each sample\n",
    "    max_pos = [tf.reduce_max(tf.slice(dists, [i * num_k, i * num_k], [num_k, num_k]), axis=1) for i in range(0, num_p)]\n",
    "    max_pos = K.concatenate(max_pos, axis=0)\n",
    "\n",
    "    # get the min inter-class distance for each sample\n",
    "    min_neg = []\n",
    "    for i in range(0, num_p):\n",
    "        left = tf.slice(dists, [i * num_k, 0], [num_k, i * num_k])\n",
    "        right = tf.slice(dists, [i * num_k, (i + 1) * num_k], [num_k, (num_p - i - 1) * num_k])\n",
    "        min_neg.append(tf.reduce_min(K.concatenate([left, right], axis=1), axis=1))\n",
    "    min_neg = K.concatenate(min_neg, axis=0)\n",
    "\n",
    "    min_max = K.concatenate([K.expand_dims(max_pos, axis=-1), K.expand_dims(min_neg, axis=-1)], axis=1)\n",
    "    return min_max, dists\n",
    "\n",
    "def get_triplet_dists(x, margin=0.5):\n",
    "    x = K.transpose(x)\n",
    "    max_pos = tf.gather(x, 0)\n",
    "    min_neg = tf.gather(x, 1)\n",
    "    # Use relu or softplus\n",
    "    L_triplet = K.expand_dims(K.softplus(margin + max_pos - min_neg), 1)\n",
    "    return L_triplet\n",
    "\n",
    "#slower speed first iteration, all following are the same speed as get_pairwise_dists\n",
    "def get_pythagorean_pairwise_dists(x,bs): #same operation as pairwise, I was just seeimg some strange results from get_pairwise_dists so I wrote this instead\n",
    "    diffs=tf.stack([tf.stack([x[j]-x[i] for i in range(bs)]) for j in range(bs)]) #just the geometric differences between vectors i.e. [x1-x2, y1-y2, z1-z2, etc.]\n",
    "    return tf.sqrt(tf.reduce_sum(diffs**2,axis=2))#differences squared (x1-x2)^2 then summed then the total square rooted by the pythagorean theorem in multiple dimensions\n",
    "\n",
    "def get_knn_sum(dists, k, bs):\n",
    "    #Now we want the lowest k distances in a given row not including the identity position\n",
    "    #I couldnt find a function for the lowest k numbers so I took the top_k of the negative distances which are also the ones closest to 0 then just negate them at the end\n",
    "    values,indices=tf.math.top_k(-dists,k=k+1) #we take k+1 because the identity (a vector minus itself is 0 and we don't want those so we grab an extra)\n",
    "    k1_dists=-values\n",
    "    #Remember to throw out the identity (do k1_dists[:,1:] to get the non-zero entries)\n",
    "    fa=tf.reduce_sum(k1_dists[:,1:],axis=1) #function a, take the sum or mean of the dists for a row\n",
    "    fb=tf.reduce_sum(fa) #function b, take the sum or mean of all the distances \n",
    "    return fb #, knn_inacc\n",
    "\n",
    "def get_acc(pdict):\n",
    "    nP = pdict['numP']\n",
    "    nK = pdict['numK']\n",
    "    margin = pdict['m']\n",
    "    k = nK-1 #I would recommend nK-1 or less unless maybe you want an idea of how close the clustering groups themselves are to one another\n",
    "    bs=nP*nK\n",
    "    \n",
    "    def s_knn(y_true, y_pred):\n",
    "        dists = get_pythagorean_pairwise_dists(y_pred,bs) #designed for get_pythagorean_pairwise_dists but works with no noticeable difference on get_pairwise_dists, just watch out for those non-zero diagonals\n",
    "        if tf.shape(dists)!=[]: #workaround for errors on initial empty arrays\n",
    "            knn_sum = get_knn_sum(dists,k,bs)\n",
    "            return knn_sum #*tf.cast(inacc+1,tf.float32)\n",
    "        else:\n",
    "            return tf.constant([0.0])\n",
    "    \n",
    "    def tr_acc(y_true, y_pred):\n",
    "        min_max, _ = get_pairwise_dists(y_pred, nP, nK)\n",
    "        loss = get_triplet_dists(min_max, margin)\n",
    "        pos = K.less(loss, .5)\n",
    "        return K.mean(pos)\n",
    "\n",
    "    return tr_acc,s_knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22e77b",
   "metadata": {
    "id": "ce22e77b"
   },
   "source": [
    "# Train the Siamese Shift encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb45b7bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5365,
     "status": "ok",
     "timestamp": 1649087575111,
     "user": {
      "displayName": "Daniel Rabayda",
      "userId": "03984531504935332105"
     },
     "user_tz": 240
    },
    "id": "bb45b7bf",
    "outputId": "a2ca8ae3-4d28-47a7-ae28-590defe7b4a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-01 16:33:53,184]\u001b[0m A new study created in memory with name: no-name-83709d9a-1f78-406e-92cb-3c9f8053a7c5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building new model:\n",
      " models/model_type_vggish:True|pat:10|d:2048x2048|od:128|n:1|ch:3|lr:0.0001|m:0.2|r_smp:44100|w_sec:2.25|vres:512|LRB:L|numP:5|numK:5|time:1654115633\n",
      "Model: \"vggish_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 193, 221, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 193, 221, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 96, 110, 64)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 110, 128)      73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 48, 55, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 48, 55, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 48, 55, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 24, 27, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 27, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 27, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 13, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 79872)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              163579904 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 2048)              0         \n",
      "                                                                 \n",
      " output_dense (Dense)        (None, 128)               262272    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172,539,392\n",
      "Trainable params: 172,539,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 16:33:53.190845: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-01 16:33:53.485118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9833 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-06-01 16:33:58.421804: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-06-01 16:33:58.837228: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-01 16:33:58.837680: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-01 16:33:58.837691: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-06-01 16:33:58.838201: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-01 16:33:58.838243: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-06-01 16:33:59.446429: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 - 32s - loss: 0.7452 - tr_acc: 0.0000e+00 - s_knn: 74.6968 - val_loss: 0.5906 - val_tr_acc: 0.0000e+00 - val_s_knn: 89.7689 - 32s/epoch - 80ms/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m params_arr\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_sec\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m0\u001b[39m,[[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m]] \n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#input each parameter in the form of ['param', init_val, [[min1,max1,step1],[min2,max2,step2],...] or [category1,category2,...],'dtype']  #dtype can be 'int','float',or'categorical'\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactory_subdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_tr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfactory\u001b[0;34m(factory_subdir, params_arr, n_tr)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tmp_acc \u001b[38;5;66;03m#this is our parameter we want to maximize\u001b[39;00m\n\u001b[1;32m    117\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfactory.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    109\u001b[0m         tmp\u001b[38;5;241m=\u001b[39mtmp[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    110\u001b[0m     param_arr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39mtmp\n\u001b[0;32m--> 111\u001b[0m tmp_pdict,tmp_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_arr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#now train a model vased on those values and pull out the accuracy and pdict\u001b[39;00m\n\u001b[1;32m    112\u001b[0m test_pdicts\u001b[38;5;241m.\u001b[39mappend(tmp_pdict) \u001b[38;5;66;03m#record every model and save it over each time\u001b[39;00m\n\u001b[1;32m    113\u001b[0m test_accuracies\u001b[38;5;241m.\u001b[39mappend(tmp_acc)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[0;34m(params_arr, acc_jm, acc_im)\u001b[0m\n\u001b[1;32m     46\u001b[0m v_step \u001b[38;5;241m=\u001b[39m v_len\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m(pdict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumP\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39mpdict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumK\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m## Train the Model\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m my_hist\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#my_hist = \u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m return_pdict\u001b[38;5;241m=\u001b[39mcopy\u001b[38;5;241m.\u001b[39mdeepcopy(pdict)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m## Test and saving of model accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1401\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   1400\u001b[0m   data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_step_from_ckpt()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m         epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m         step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m         _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m       callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/data_adapter.py:1248\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1249\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1250\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m   1253\u001b[0m     original_spe)\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Folders\n",
    "model_folder = vdir('models')\n",
    "log_folder = vdir('logs')\n",
    "\n",
    "def train_test_model(params_arr,acc_jm=10,acc_im=10):\n",
    "    \n",
    "    # Model Parameters\n",
    "    pdict = {}\n",
    "    pdict['vggish'] = True\n",
    "    pdict['pat'] = 10 #default 10\n",
    "    pdict['d'] = [2048, 2048] #default [256,128]\n",
    "    pdict['od'] = 128 #default 128, better results with higher od\n",
    "    pdict['n'] = 1\n",
    "    pdict['ch'] = 3\n",
    "    \n",
    "    pdict['lr'] = 0.0001 #default 0.0001, best around 0.0001 with everything elseat default\n",
    "    pdict['m'] = 0.2 #default 0.2\n",
    "    #pdict['a'] = 0.0000001\n",
    "\n",
    "    # Generator Parameters\n",
    "    pdict['r_smp'] = 44100\n",
    "    pdict['w_sec'] = 2 #default 2\n",
    "    pdict['vres'] = 2**9 #default 2**9\n",
    "    pdict['LRB']='L' #default 'L'\n",
    "    pdict['numP'] = 5 #default 5\n",
    "    pdict['numK'] = 5 #default 5\n",
    "    \n",
    "    for param_arr in params_arr:\n",
    "        [param,val,_,_]=param_arr\n",
    "        pdict[param] = val #this overrides the given parameter's default value\n",
    "\n",
    "    # build the generators\n",
    "    gen_trn = generatorC(smallData, pdict, smallLabels, smallTimes, smallClassgroups) #gen1s smallClassgroups\n",
    "    gen_val = generatorC(smallData2, pdict, smallLabels2, smallTimes2, smallClassgroups2)\n",
    "\n",
    "    # load the model\n",
    "    pdict['time'] = int(time.time())\n",
    "    #pdict['iniW'] = 1651599114\n",
    "    model, model_name = load_custom_model(pdict)\n",
    "    model.summary()\n",
    "\n",
    "    # define the steps per epoch\n",
    "    t_len = 10000 #(len(dataTrain)*3)*dataTrain[0].shape[0]/pdict['r_smp']/pdict['w_sec'] #one file divided by window size  (len(dataTrain)/2)*\n",
    "    v_len = t_len/10 #t_len/10\n",
    "    t_step = t_len//(pdict['numP']*pdict['numK'])+1  #t_len over batch size plus one so we never have a t_step of 0\n",
    "    v_step = v_len//(pdict['numP']*pdict['numK'])+1\n",
    "\n",
    "\n",
    "    ## Train the Model\n",
    "    my_hist=model.fit(gen_trn, steps_per_epoch=t_step, epochs=100, #my_hist = \n",
    "                  validation_data=gen_val, validation_steps=v_step, verbose=2,\n",
    "                  callbacks=get_callbacks(model_name, model_folder, log_folder))\n",
    "\n",
    "    return_pdict=copy.deepcopy(pdict)\n",
    "\n",
    "\n",
    "    ## Test and saving of model accuracy\n",
    "    pdict['numP'] = 18 #must use 18 first go around, because the next generated y_pred2 will have all different classes and if all the classes are not in y_pred then this will make it miss values just because it doesn't know them\n",
    "    pdict['numK'] = 10\n",
    "    train_gen=generatorC(smallData,pdict,smallLabels,smallTimes,smallClassgroups)\n",
    "    o_acc=[]\n",
    "    for j in range(acc_jm):\n",
    "        spectrograms,ls=next(train_gen)\n",
    "        shuffled=np.random.permutation(len(ls)) #shuffle everything just incase\n",
    "        spectrograms=spectrograms[shuffled]\n",
    "        ls=ls[shuffled]\n",
    "        y_pred = model.predict(spectrograms)\n",
    "\n",
    "        k=9\n",
    "        kclass=KNeighborsClassifier(n_neighbors=k)\n",
    "        kclass.fit(y_pred,ls)\n",
    "\n",
    "        acc=[]\n",
    "        pdict['numP'] = 10 #these can be anything but 10x10 is sufficient\n",
    "        pdict['numK'] = 10\n",
    "        \n",
    "        test_gen=generatorC(smallData3,pdict,smallLabels3,smallTimes3,smallClassgroups3)\n",
    "        for i in range(acc_im):\n",
    "            spectrograms2,ls2=next(test_gen)\n",
    "            shuffled2=np.random.permutation(len(ls2))\n",
    "            spectrograms2=spectrograms2[shuffled2]\n",
    "            ls2=ls2[shuffled2]\n",
    "            y_pred2 = model.predict(spectrograms2)\n",
    "            ls2_pred= kclass.predict(y_pred2)\n",
    "            acc.append((ls2_pred==ls2).mean())\n",
    "\n",
    "        #print('Prediction Average Accuracy:',np.mean(acc))#,' std:',np.std(acc))\n",
    "        o_acc.append(np.mean(acc))\n",
    "    return_accuracy=np.mean(o_acc)\n",
    "    print('Overall Accuracy of Current Model:',return_accuracy) #The goal here is anything above 1/18~=5.6%, 1/18 is the probability of guessing the right class randomly from 18 classes\n",
    "    return return_pdict,return_accuracy\n",
    "\n",
    "def factory(factory_subdir,params_arr,n_tr=10):\n",
    "    test_accuracies=[] #initialize these\n",
    "    test_pdicts=[]\n",
    "    def objective(trial):\n",
    "        for param_arr in params_arr: #go one by one through the parameters and suggest new values (depending on data type)\n",
    "            tmp=[]\n",
    "            [name,_,ranges,dtype]=param_arr\n",
    "            if dtype=='categorical': tmp.append(trial.suggest_categorical(name, ranges))\n",
    "            else:\n",
    "                for i,rn in enumerate(ranges):\n",
    "                    ind_str=str(i) if len(ranges)>1 else '' #this is because we need different \"parameter\" names for different values in optuna\n",
    "                    if dtype=='int':\n",
    "                        tmp.append(trial.suggest_int(name+ind_str, rn[0], rn[1], step=rn[2]))\n",
    "                    else:\n",
    "                        tmp.append(trial.suggest_float(name+ind_str, rn[0], rn[1], step=rn[2]))\n",
    "            if len(tmp)==1: #formats everything but lists as just their value\n",
    "                tmp=tmp[0]\n",
    "            param_arr[1]=tmp\n",
    "        tmp_pdict,tmp_acc=train_test_model(params_arr) #now train a model vased on those values and pull out the accuracy and pdict\n",
    "        test_pdicts.append(tmp_pdict) #record every model and save it over each time\n",
    "        test_accuracies.append(tmp_acc)\n",
    "        np.save(factory_subdir+'/test_accuracies.npy',test_accuracies)\n",
    "        np.save(factory_subdir+'/test_pdicts.npy',test_pdicts)\n",
    "        return tmp_acc #this is our parameter we want to maximize\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_tr)\n",
    "    study.best_params\n",
    "\n",
    "    \n",
    "\n",
    "# The Factory\n",
    "factory_folder = vdir(more_folder+'/factory')\n",
    "factory_subdir = vdir(factory_folder+'/vggish_varied_wsec2') #change this every time you start a new assembly line\n",
    "params_arr=[['w_sec',0,[[0.75,3.75,0.25]],'float']]\n",
    "#input each parameter in the form of ['param', init_val, [[min1,max1,step1],[min2,max2,step2],...] or [category1,category2,...],'dtype']  #dtype can be 'int','float',or'categorical'\n",
    "\n",
    "factory(factory_subdir,params_arr,n_tr=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a27293e-88e7-4fb7-a148-5b0a80bc2ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0795 0.0858 0.0925 0.0967 0.0813 0.0591 0.063  0.0895 0.0807 0.0858\n",
      " 0.0886 0.075  0.078 ]\n"
     ]
    }
   ],
   "source": [
    "factory_folder = more_folder+'/factory'\n",
    "factory_subdir = factory_folder+'/vggish_varied_wsec2'\n",
    "\n",
    "test_accuracies=np.load(factory_subdir+'/test_accuracies.npy')\n",
    "test_pdicts=np.load(factory_subdir+'/test_pdicts.npy',allow_pickle=True)\n",
    "print(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "262cabc5-fbe6-4d5c-84e2-36027a26aa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>w_sec</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1654121075</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1654122115</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1654125762</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1654126732</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1654115735</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1654123371</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1654120301</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.0813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1654117450</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1654123876</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1654124630</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1654122629</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1654118169</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1654119002</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0         time w_sec     acc\n",
       "6   1654121075   2.5  0.0591\n",
       "7   1654122115  2.25   0.063\n",
       "12  1654125762   2.0   0.075\n",
       "13  1654126732  3.25   0.078\n",
       "1   1654115735  3.75  0.0795\n",
       "9   1654123371  2.75  0.0807\n",
       "5   1654120301  2.25  0.0813\n",
       "2   1654117450  3.75  0.0858\n",
       "10  1654123876  3.25  0.0858\n",
       "11  1654124630   4.0  0.0886\n",
       "8   1654122629   3.0  0.0895\n",
       "3   1654118169   2.0  0.0925\n",
       "4   1654119002  3.75  0.0967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params=['time','w_sec']\n",
    "df=pd.DataFrame([params+['acc']]+[[dicti[param] for param in params]+[test_accuracies[i]] for i,dicti in enumerate(test_pdicts)])\n",
    "new_header = df.iloc[0]; df = df[1:]\n",
    "df.columns = new_header\n",
    "df=df.sort_values(by=['acc'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e66d6b-7f0e-4624-84e6-227c179ff79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(df['acc'])\n",
    "y=np.array(list(df['w_sec']))#[:,0]\n",
    "z=np.polyfit(x,y,1)\n",
    "p=np.poly1d(z)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,p(x)) #'d' [500, 400], 'w_sec' 3.5, 'od' 500"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b0bd3ee2",
    "d45a5f79",
    "07e85bfa",
    "1222f42a-0d69-4587-b3e9-eac7f408121f",
    "792e9feb-807e-4901-8725-7ad05287127f",
    "f8321602"
   ],
   "name": "NoiseNet_20220325_DPR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
